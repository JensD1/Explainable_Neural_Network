# # -*- coding: utf-8 -*-
# """ExplainableMLP.ipynb
#
# Automatically generated by Colaboratory.
#
# Original file is located at
#     https://colab.research.google.com/drive/1WpARn39EbSorrbZXSSSY2PoLLr-4NM8T
#
# # **Bachelor Proef**
#
# ## 1) Initialisation
#
# *Ik zal om te beginnen trachten de al bestaande state of the art netwerken (CNN) te explainen. Een tool dat ik hiervoor alvast gebruik is flashtorch, wat teruggevonden kan worden op de GitHub pagina:* https://github.com/MisaOgura/flashtorch
#
# ### Installation and import statements
# """
#
# # Commented out IPython magic to ensure Python compatibility.
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
#
# import torchvision
# from torchvision import transforms, datasets
# from torch.utils.data import DataLoader
# import matplotlib.pyplot as plt
#
# import torchvision.models as models
#
# # %matplotlib inline
# # %config InlineBackend.figure_format = 'retina'
#
# import copy
#
#
# """### Class declaration
#
# #### MLP
#
# Hier definieer ik mijn eigen neuraal netwerken. Deze dienen om te testen om een MLP te kunnen explainen. Het verschil tussen de twee is dat bij de NeuralNetSeq alles in verschillende nn.Sequential modules geplaatst is. Op deze manier kan ik testen of mijn code algemeen genoeg geschreven is.
# """
#
# """#### Manager
#
# Deze klasse heb ik aangemaakt zodat ik vanuit deze in de toekomst verschillende netwerken kan binnenladen, trainen en explainen.
# """
#

# """#### MLP Explainer
#
# Deze klasse zal door de Manager gebruikt worden om lineare neurale netwerken te verklaren.
# """
#
#
# """## 2) Get images and models
#
# ### Images
#
# Getting some images from GitHub:
# """
#
# # Download example images
#
# !mkdir -p images
#
# !wget -nv \
#     https://github.com/MisaOgura/flashtorch/raw/master/examples/images/great_grey_owl.jpg \
#     https://github.com/MisaOgura/flashtorch/raw/master/examples/images/peacock.jpg        \
#     https://github.com/MisaOgura/flashtorch/raw/master/examples/images/toucan.jpg         \
#     -P /content/images
#
# """load images so we can give them to our neural network manager."""
#
# image = load_image('/content/images/great_grey_owl.jpg')
# image2 = load_image('/content/images/peacock.jpg')
#
# fig = plt.figure()
# a = fig.add_subplot(1, 2, 1)
# plt.imshow(image)
# a.set_title('Original image: Great grey owl')
# a.axis('off');
# a = fig.add_subplot(1, 2, 2)
# plt.imshow(image2)
# a.set_title('Original image: Peacock')
# a.axis('off');
#
# """### Models
#
# Loading some models to test our neural network manager.
# """
#
# model_alexnet = models.alexnet(pretrained=True)
# model_densenet = models.densenet201(pretrained=True)
# model_vgg = models.vgg16(pretrained=True)
#
#
# """## 3) Test code that works (demo)
#
# #### Netwerk managers initialiseren en netwerken trainen of inladen
#
# Train the models model1 and model2
# """
#
# #lossFunction = nn.NLLLoss() #negative log likelihood loss
# #optimizer1 = optim.SGD(model1.parameters(), lr=0.003, momentum=0.9)
# #optimizer2 = optim.SGD(model2.parameters(), lr=0.003, momentum=0.9)
# #num_epochs = 10
# #for epoch in range(num_epochs):
# #    loss_ = 0
# #    for images, labels in train_loader:
# #        # Flatten the input images of [28,28] to [1,784]
# #        images = images.reshape(-1, 28*28)
# #
# #        # Forward Pass
# #        output = model1(images)
# #        # Loss at each iteration by comparing to target(label)
# #        loss = lossFunction(output, labels)
# #
# #        # Backpropogating gradient of loss
# #        optimizer1.zero_grad()
# #        loss.backward()
# #
# #        # Updating parameters(weights and bias)
# #        optimizer1.step()
# #
# #        loss_ += loss.item()
# #    print("Epoch{}, Training loss:{}".format(epoch, loss_ / len(train_loader)))
# #    num_epochs = 10
# #for epoch in range(num_epochs):
# #    loss_ = 0
# #    for images, labels in train_loader:
# #        # Flatten the input images of [28,28] to [1,784]
# #        images = images.reshape(-1, 784)
# #
# #        # Forward Pass
# #        output = model2(images)
# #        # Loss at each iteration by comparing to target(label)
# #        loss = lossFunction(output, labels)
# #
# #        # Backpropogating gradient of loss
# #        optimizer2.zero_grad()
# #        loss.backward()
# #
# #        # Updating parameters(weights and bias)
# #        optimizer2.step()
# #
# #        loss_ += loss.item()
# #    print("Epoch{}, Training loss:{}".format(epoch, loss_ / len(train_loader)))
#
# """Save model 1 and 2"""
#
# # torch.save(model1.state_dict(), "mnist_model.pt")
# # torch.save(model2.state_dict(), "mnist_model_seq.pt")
#
# """Load model 1 and 2"""
#
# model1.load_state_dict(torch.load("mnist_model.pt"))
# model2.load_state_dict(torch.load("mnist_model_seq.pt"))
#
# """Accuracy test"""
#
# with torch.no_grad():
#     correct = 0
#     total = 0
#     for images, labels in test_loader:
#         images = images.reshape(-1, 784)
#         out = model1(images)
#         _, predicted = torch.max(out, 1)
#         total += labels.size(0)
#         correct += (predicted == labels).sum().item()
#     print('Testing accuracy: {} %'.format(100 * correct / total))
#     correct = 0
#     total = 0
#     for images, labels in test_loader:
#         images = images.reshape(-1, 784)
#         out = model2(images)
#         _, predicted = torch.max(out, 1)
#         total += labels.size(0)
#         correct += (predicted == labels).sum().item()
#     print('Testing accuracy: {} %'.format(100 * correct / total))
#
# """creating 5 neural network managers, each one of them has another model."""
#
# #netw = NNManager(model_alexnet)
# #netw2 = NNManager(model_densenet)
# #netw3 = NNManager(model_vgg)
# mlp1 = NNManager(model1)
# mlp2 = NNManager(model2)
#
# """### Saliency
#
# Saliency map with an owl (while using the alexnet model).
# This gives us two rows, the first one is without guidance and the second one with guidance (see: `Striving for Simplicity: The All Convolutional Net <https://arxiv.org/pdf/1412.6806.pdf>`).
# """
#
# netw.saliency_map(image, 24, guided = False, use_gpu= True)
# netw.saliency_map(image, 24, guided = True, use_gpu= True)
#
# """Saliency map but with a peacock (with the alexnet model)."""
#
# netw.saliency_map(image2, 84, guided = False, use_gpu= True)
# netw.saliency_map(image2, 84, guided = True, use_gpu= True)
#
# """Saliency map from both an own and a peacock while using the densenet model."""
#
# netw2.saliency_map(image, 24, guided = False, use_gpu = True)
# netw2.saliency_map(image, 24, guided = True, use_gpu = True)
# netw2.saliency_map(image2, 84, guided = False, use_gpu = True)
# netw2.saliency_map(image2, 84, guided = True, use_gpu = True)
#
# """### Activation_maximisation
#
# #### *all Convolutional layers*
#
# This is the most general form for activation maximisation. Every convolutional layer will have a turn to show 4 random filters. (4 is the default number of features in flashtorch.
# """
#
# netw.activation_maximisation(use_gpu=True)
#
# """The code below will give the same result as above, but ofcoarse with some randomness in the filters."""
#
# netw.activation_maximisation(use_gpu=True, random=True)
#
# """Now we will select the filters that we want to see."""
#
# netw.activation_maximisation(use_gpu=True, filters=[2, 20])
#
# """#### *One convolutional layer*
#
# From now on we select the convolutional layer on which we will preform activation maximisation.
# """
#
# # list(model_alexnet.features.named_children()) #f.e. layer 3 is a conv2d layer
# netw.activation_maximisation(use_gpu=True, conv_layer_int=3)
#
# """Now we will specify which filter we want to see."""
#
# #list(model_vgg.features.named_children()) # f.e. layer 28 is a convolutional layer
# netw3.activation_maximisation(use_gpu=True, conv_layer_int=28, filters=5)
#
# """Finally, we will create a deepdream."""
#
# netw3.deepdream('/content/images/great_grey_owl.jpg', 28, 24, use_gpu=True)
#
# """## 4) Test environment for new code
#
# *Single command tests* (indien ik snel iets wil uitproberen of een current state wil tonen):
# """
#
# #print(model.state_dict) #children & modules & parameters doen +- hetzelfde
# #for _,module in model.named_modules():
# #  if isinstance(module, nn.Conv2d):
# #    print(module)
# #print(model._modules.items())
#
# netw2.activation_maximisation(use_gpu=True)
#
# # Commented out IPython magic to ensure Python compatibility.


import copy

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torchvision import transforms, datasets

import NeuralNet as net
import NeuralNetSeq as netSeq
import LRP as layerwiseRelevancePropagation

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

trainset = datasets.MNIST('', download=True, train=True, transform=transforms.ToTensor())
testset = datasets.MNIST('', download=True, train=False, transform=transforms.ToTensor())

train_loader = DataLoader(trainset, batch_size=64, shuffle=True)
test_loader = DataLoader(testset, batch_size=64, shuffle=True)

# class MLPExlainer():

"""create MLP models"""

model1 = net.NeuralNet(28 * 28, [128, 64], 10)
model2 = netSeq.NeuralNetSeq()
model1.load_state_dict(torch.load("mnist_model.pt"))
model2.load_state_dict(torch.load("mnist_model_seq.pt"))
lrp1 = layerwiseRelevancePropagation.LRP(model1)
lrp2 = layerwiseRelevancePropagation.LRP(model2)

"""code runnen: hooks worden eerst geplaatst (op een copie zodat deze niet opgeslagen worden op het oorspronkelijk 
model) Vervolgens zullen we een forward functie uitproberen. """

for images, labels in test_loader:
    relevance = lrp1.lrp(images[0].view(-1, 28 * 28), debug=True, _return=True, rho="lin")
    relevance2 = lrp1.lrp(images[0].view(-1, 28 * 28), debug=True, _return=True, rho="relu")
    break
